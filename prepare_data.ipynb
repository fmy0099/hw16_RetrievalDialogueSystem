{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='TextRNN text classifier')\n",
    "\n",
    "parser.add_argument('-in-train-path', type=str, default='data/train.txt', help='训练数据路径')\n",
    "parser.add_argument('-out-train-path', type=str, default='data/train_split.txt', help='训练数据分词后的路径')\n",
    "parser.add_argument('-train-file-count', type=int, default=39, help='切分后的训练数据文件数目')\n",
    "parser.add_argument('-valid-file-count', type=int, default=1, help='切分后的验证数据文件数目')\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "文档分词\n",
    "'''\n",
    "def split_train_file(in_train_path,out_train_path):\n",
    "    print('split_train_file')\n",
    "    out = open(out_train_path, 'w',encoding='utf-8') \n",
    "    line_no = 0\n",
    "    with open(in_train_path, 'r',encoding='utf-8') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            words = tokenizer(line)\n",
    "            line_new = ''\n",
    "            for w in words:\n",
    "                line_new += w + ' '\n",
    "            line_new += '\\n'\n",
    "            if len(line_new.strip())==0 and len(line.strip())>0:\n",
    "                line_new = 'pad\\n'\n",
    "            out.write(line_new)\n",
    "            line = file.readline()\n",
    "            if line_no%1000==0:\n",
    "                sys.stdout.write('\\rsplit_train_file line_no = [{}]'.format(line_no))\n",
    "            line_no += 1\n",
    "    out.close()\n",
    "    print('split_train_file finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "加载停用词\n",
    "'''\n",
    "def get_stop_words():\n",
    "    file = open('data/stopwords-iso.json', 'r',encoding='utf-8') \n",
    "    stop_words = json.loads(file.read())['zh']   \n",
    "    file.close() \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "'''\n",
    "分词\n",
    "'''\n",
    "def tokenizer(text): \n",
    "    regex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9]')\n",
    "    text = regex.sub(' ', text)\n",
    "    text = text.strip()    \n",
    "    return [word.strip() for word in jieba.cut(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "过滤停用词\n",
    "'''\n",
    "def stop_filter(words,stop_words):\n",
    "    ret = []\n",
    "    for word in words:\n",
    "        if len(word.strip())>0 and (word not in stop_words):\n",
    "            ret.append(word)\n",
    "    return ret   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "拼接句子\n",
    "'''\n",
    "def make_line(words):\n",
    "    ret = ''\n",
    "    for word in words:\n",
    "        ret += word.strip() + ' '\n",
    "    if len(ret.strip())==0:\n",
    "        ret = 'pad'\n",
    "    return ret + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "构建词汇表\n",
    "'''\n",
    "def make_vocab(in_train_path,stop_words):\n",
    "    vocab = {}\n",
    "    line_no = 0\n",
    "    \n",
    "    with open(in_train_path, 'r',encoding='utf-8') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            words = [word.strip() for word in line.split()]\n",
    "            words = stop_filter(words,stop_words)\n",
    "            for w in words:\n",
    "                if w in vocab.keys():\n",
    "                    vocab[w] = vocab[w]+1\n",
    "                else:\n",
    "                    vocab[w] = 1\n",
    "            if line_no%1000==0:\n",
    "                sys.stdout.write('\\rline_no = [{}]'.format(line_no))\n",
    "            line = file.readline()\n",
    "            line_no+=1 \n",
    "     \n",
    "    vocab = dict(sorted(vocab.items(), key = lambda kv:(kv[1], kv[0]),reverse=True))\n",
    "    print('vocab size = {}'.format(len(vocab)))\n",
    "          \n",
    "    stoi = {}\n",
    "    vocab_list = list(vocab.keys())\n",
    "    for i in range(len(vocab_list)):\n",
    "        w = vocab_list[i]\n",
    "        stoi[w] = i+2 #unk pad\n",
    "           \n",
    "    vocab_file = open('data/vocab.txt','w',encoding='utf-8')\n",
    "    txt = 'name,freq\\n'+'unk,10000000\\n'+'pad,10000000\\n'\n",
    "    \n",
    "    for w in vocab.keys():\n",
    "        txt += w + ',' + str(vocab[w]) + '\\n'\n",
    "        \n",
    "    vocab_file.write(txt)\n",
    "    vocab_file.close()\n",
    "    \n",
    "    print('vocab finished ')\n",
    "\n",
    "    return vocab,stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import math\n",
    "\n",
    "def txt2csv(in_train_path,stop_words):\n",
    "    session = []\n",
    "    replys = []\n",
    "    out_train_path,out_valid_path,path = '','',''\n",
    "    session_count = 0\n",
    "    with open(in_train_path, 'r',encoding='utf-8') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            if line == '\\n': \n",
    "                session_count+=1 \n",
    "                replys.append(reply)\n",
    "            else:\n",
    "                reply = line.replace(',','').strip()\n",
    "            line = file.readline()  \n",
    "     \n",
    "    count_per_file = session_count/(train_file_count + valid_file_count)\n",
    "    print('reply count = {} per count = {} '.format(len(replys),count_per_file))\n",
    "    session_no = 0\n",
    "    with open(in_train_path, 'r',encoding='utf-8') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            if line == '\\n':         \n",
    "                context = ''\n",
    "                for i in range(len(session)-2):\n",
    "                    context += session[i]\n",
    "                \n",
    "                context = make_line(stop_filter(context.split(),stop_words)).strip()\n",
    "                query = make_line(stop_filter(session[-2].split(),stop_words)).strip()\n",
    "                reply = make_line(stop_filter(session[-1].split(),stop_words)).strip()\n",
    "                neg_reply1 = make_line(stop_filter(replys[random.randint(0,len(replys)-1)].split(),stop_words)).strip()\n",
    "    \n",
    "                session_no += 1\n",
    "                file_no = math.floor(session_no/count_per_file) + 1\n",
    "                if file_no <= train_file_count:\n",
    "                    path = 'data/train/train{}.csv'.format(file_no)\n",
    "                    if path != out_train_path:\n",
    "                        out_train_path = path\n",
    "                        out_train_file = open(out_train_path,'w',encoding='utf-8')\n",
    "                        out_train_file.write('label,context+query,reply\\n')                                               \n",
    "                        out_file = out_train_file\n",
    "                else:\n",
    "                    path = 'data/train/valid{}.csv'.format(file_no - train_file_count)\n",
    "                    if path != out_valid_path:\n",
    "                        out_valid_path = path\n",
    "                        out_valid_file = open(out_valid_path,'w',encoding='utf-8')\n",
    "                        out_valid_file.write('label,context+query,reply\\n')                                             \n",
    "                        out_file = out_valid_file\n",
    "                    \n",
    "                out_file.write('{},{}，{},{}\\n'.format(1,context,query,reply))\n",
    "                out_file.write('{},{}，{},{}\\n'.format(0,context,query,neg_reply1))\n",
    "                \n",
    "                if session_no%500==0:\n",
    "                    sys.stdout.write('\\rpath={} session_no={}/{}\\r'.format(path,session_no,session_count))\n",
    "                \n",
    "                session = []\n",
    "            else:\n",
    "                session.append(line.replace(',','').strip())   \n",
    "            line = file.readline() \n",
    "    out_valid_file.close()\n",
    "    out_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    #加载停用词\n",
    "    stop_words = get_stop_words()\n",
    "#     stop_words = []\n",
    "\n",
    "    #切分train.txt，39个训练数据文件 + 1个验证数据文件\n",
    "    split_train_file(args.in_train_path,args.out_train_path)\n",
    "    \n",
    "    #生成词汇表\n",
    "    vocab,stoi = make_vocab(args.out_train_path,stop_words)\n",
    "    \n",
    "    #训练数据集和验证数据集分词后转成csv文件\n",
    "    txt2csv(args.out_train_path,stop_words)\n",
    "    \n",
    "    #测试数据集分词\n",
    "    split_train_file('data/seq_replies.txt','data/seq_replies_split.txt')\n",
    "    split_train_file('data/seq_context.txt','data/seq_context_split.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
