{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "import re\n",
    "from torchtext.vocab import Vectors\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import json\n",
    "import threading\n",
    "\n",
    "parser = argparse.ArgumentParser(description='RNN QA')\n",
    "\n",
    "parser.add_argument('-lr', type=float, default=0.01, help='学习率')\n",
    "parser.add_argument('-batch-size', type=int, default=128)\n",
    "parser.add_argument('-context-len', type=int, default=200)\n",
    "parser.add_argument('-epoch', type=int, default=1)\n",
    "parser.add_argument('-embedding-dim', type=int, default=300, help='词向量的维度')\n",
    "parser.add_argument('-hidden_size', type=int, default=128, help='lstm中神经单元数')\n",
    "parser.add_argument('-layer-num', type=int, default=1, help='lstm stack的层数')\n",
    "parser.add_argument('-bidirectional', type=bool, default=True, help='是否使用双向lstm')\n",
    "parser.add_argument('-static', type=bool, default=True, help='是否使用预训练词向量')\n",
    "parser.add_argument('-fine-tune', type=bool, default=True, help='预训练词向量是否要微调')\n",
    "parser.add_argument('-log-interval', type=int, default=1, help='经过多少iteration记录一次训练状态')\n",
    "parser.add_argument('-test-interval', type=int, default=100, help='经过多少iteration对验证集进行测试')\n",
    "parser.add_argument('-save-best', type=bool, default=True, help='当得到更好的准确度是否要保存')\n",
    "parser.add_argument('-save-dir', type=str, default='model_dir', help='存储训练模型位置')\n",
    "parser.add_argument('-vocab-path', type=str, \n",
    "                    default='D:/Summer/DeepL-data/glove.840B.300d/glove.840B.300d.txt', \n",
    "                    help='词向量,static为True时生效')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "停用词\n",
    "'''\n",
    "def get_stop_words():\n",
    "    file = open('data/stopwords-iso.json', 'r',encoding='utf-8') \n",
    "    stop_words = json.loads(file.read())['zh']   \n",
    "    file.close() \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "分词\n",
    "'''\n",
    "def tokenizer(text): \n",
    "    all_words = [word for word in text.split()]\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "加载词汇表\n",
    "'''\n",
    "def load_vocab(args):\n",
    "    print('Load vocab...')\n",
    "#     text = data.Field(sequential=True,lower=True, stop_words=args.stop_words, tokenize=tokenizer)\n",
    "    text = data.Field(sequential=False,lower=True,tokenize=tokenizer)\n",
    "    label = data.Field(sequential=True)\n",
    "    \n",
    "    txt_vocab = data.TabularDataset.splits(\n",
    "            path='data/',\n",
    "            skip_header=True,\n",
    "            train='vocab.txt',\n",
    "            format='csv',\n",
    "            fields=[('name',text),('freq',label)],\n",
    "        )[0]\n",
    "#     text.build_vocab(txt_vocab,max_size=420000,vectors=Vectors(name = args.vocab_path)) \n",
    "    text.build_vocab(txt_vocab,vectors=Vectors(name = args.vocab_path))\n",
    "    \n",
    "    vocab_iter = data.Iterator(\n",
    "            txt_vocab,\n",
    "            sort_key=lambda x: len(x.label),\n",
    "            batch_size=len(txt_vocab), \n",
    "            device=-1\n",
    "    )\n",
    "    \n",
    "    args.embedding_dim = text.vocab.vectors.size()[-1]\n",
    "    args.vectors = text.vocab.vectors    \n",
    "    args.stoi = vars(text.vocab)['stoi']    \n",
    "    args.itos = vars(text.vocab)['itos']    \n",
    "    args.vocab_size = len(text.vocab)   \n",
    "    args.label_num = 2\n",
    "    \n",
    "#     print(list(args.stoi)[:10])\n",
    "    \n",
    "    print('vocab size : ',args.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "当前训练数据文件的词，映射到词汇表中\n",
    "'''\n",
    "def cvt_dict(args,batch):\n",
    "    for i in range(batch.size(0)):\n",
    "        for j in range(batch.size(1)):\n",
    "            index = batch[i][j]\n",
    "            name = args.cur_itos[index]\n",
    "            if name in args.stoi.keys():\n",
    "                batch[i][j] = args.stoi[name]\n",
    "            else:\n",
    "                batch[i][j] = args.stoi[0]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "加载训练数据文件和验证数据文件\n",
    "'''\n",
    "def load_data(args):\n",
    "    print('加载数据:{}'.format(args.train_file))\n",
    "       \n",
    "    text = data.Field(sequential=True, fix_length=args.context_len, lower=True, tokenize=tokenizer)\n",
    "    label = data.Field(sequential=False)\n",
    "    \n",
    "    train, val = data.TabularDataset.splits(\n",
    "            path='data/train/',\n",
    "            skip_header=True,\n",
    "            train=args.train_file,\n",
    "            validation='valid.csv',\n",
    "            format='csv',\n",
    "            fields=[('label', label), ('query', text),('reply', text)],\n",
    "        ) \n",
    "\n",
    "    if args.static:\n",
    "        text.build_vocab(train, val, vectors=Vectors(name = args.vocab_path)) \n",
    "        args.cur_itos = vars(text.vocab)['itos']\n",
    "    else: \n",
    "        text.build_vocab(train, val)\n",
    "\n",
    "    label.build_vocab(train, val)\n",
    "\n",
    "    train_iter, val_iter = data.Iterator.splits(\n",
    "            (train, val),\n",
    "            sort_key=lambda x: len(x.query),\n",
    "            batch_sizes=(args.batch_size, args.batch_size*2), \n",
    "            device=-1\n",
    "    )\n",
    "    \n",
    "    args.label_num = len(label.vocab)-1\n",
    "    print(vars(train.examples[0]))\n",
    "\n",
    "    return train_iter, val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(TextRNN, self).__init__()\n",
    "        embedding_dim = args.embedding_dim\n",
    "        label_num = args.label_num\n",
    "        vocab_size = args.vocab_size\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.layer_num = args.layer_num\n",
    "        self.bidirectional = args.bidirectional\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if args.static:  \n",
    "            v = Vectors(name = args.vocab_path)\n",
    "            self.embedding = self.embedding.from_pretrained(args.vectors, freeze=not args.fine_tune)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, # x的特征维度,即embedding_dim\n",
    "                            self.hidden_size,# 隐藏层单元数\n",
    "                            self.layer_num,# 层数\n",
    "                            batch_first=True,# 第一个维度设为 batch, 即:(batch_size, seq_length, embedding_dim)\n",
    "                            bidirectional=self.bidirectional) # 是否用双向\n",
    "        self.fc = nn.Linear(self.hidden_size * 2 , label_num) \n",
    "        \n",
    "    \n",
    "    def forward(self, query,reply): \n",
    "        x = torch.cat((query,reply), 0)             \n",
    "        x = self.embedding(x)      \n",
    "        h0 = torch.zeros(self.layer_num * 2, x.size(0), self.hidden_size) if self.bidirectional else torch.zeros(self.layer_num, x.size(0),self.hidden_size)\n",
    "        c0 = torch.zeros(self.layer_num * 2, x.size(0), self.hidden_size) if self.bidirectional else torch.zeros(self.layer_num, x.size(0),self.hidden_size)       \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out1,out2 = torch.split(out, [query.size(0),reply.size(0)], dim=0)          \n",
    "        out1 = torch.einsum('bik->bki',out1)\n",
    "        out1 = torch.einsum('bik,kj->bij',out1,self.w1)\n",
    "        out3 = torch.einsum('bki,bij->bj', out1,out2)\n",
    "        out = self.fc(out3[:, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args,train_iter, dev_iter):\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, args.epoch + 1):            \n",
    "        for batch in train_iter:\n",
    "            f1,f2, target = batch.query,batch.reply,batch.label\n",
    "           \n",
    "            with torch.no_grad():\n",
    "                f1.t_()\n",
    "                f2.t_()\n",
    "                target.sub_(1)\n",
    "     \n",
    "            optimizer.zero_grad()           \n",
    "            logits = model(cvt_dict(args,f1),cvt_dict(args,f2))            \n",
    "            model.loss = F.cross_entropy(logits, target)\n",
    "            model.loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % args.log_interval == 0:\n",
    "                corrects = (torch.max(logits, 1)[1] == target).sum()\n",
    "                train_acc = 100.0 * corrects / batch.batch_size\n",
    "                sys.stdout.write(\n",
    "                    '\\rEpoch[{}] Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(epoch,steps,\n",
    "                                                                             model.loss.item(),\n",
    "                                                                             train_acc,\n",
    "                                                                             corrects,\n",
    "                                                                             batch.batch_size))\n",
    "            if steps % args.test_interval == 0:\n",
    "                dev_acc = eval(dev_iter, model, args)\n",
    "                if dev_acc > model.best_acc:\n",
    "                    model.best_acc = dev_acc\n",
    "                    if args.save_best:\n",
    "                        print('Saving best model, acc: {:.4f}%\\n'.format(model.best_acc))\n",
    "                        save(model,optimizer,args.save_dir, 'best', steps)\n",
    "\n",
    "    print('\\rtrain finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "验证、测试\n",
    "'''\n",
    "def eval(data_iter, model, args):\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in data_iter:\n",
    "        f1,f2, target = batch.query,batch.reply,batch.label          \n",
    "        with torch.no_grad():\n",
    "            f1.t_()\n",
    "            f2.t_()\n",
    "            target.sub_(1)\n",
    "        logits = model(f1,f2)\n",
    "        pre = torch.max(logits, 1)[1]\n",
    "        \n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logits, 1)\n",
    "                     [1].view(target.size()) == target).sum()\n",
    "    size = len(data_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss,\n",
    "                                                                       accuracy,\n",
    "                                                                       corrects,\n",
    "                                                                       size))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model,optimizer,save_dir,save_prefix,steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "    \n",
    "    save_path = '{}.pt'.format(save_prefix)\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': model.loss,\n",
    "            'w1':model.w1,\n",
    "            'best_acc':model.best_acc,\n",
    "            }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(args):\n",
    "    model = TextRNN(args)\n",
    "    model.best_acc = 0\n",
    "    dtype = torch.FloatTensor\n",
    "#     model.w1 = Variable(torch.randn(args.hidden_size * 2, args.hidden_size * 2).type(dtype), requires_grad=True) if args.bidirectional else Variable(torch.randn(args.hidden_size, 100).type(dtype), requires_grad=True)\n",
    "    model.w1 = Variable(torch.randn(args.context_len,args.context_len).type(dtype), requires_grad=True) if args.bidirectional else Variable(torch.randn(args.hidden_size, 100).type(dtype), requires_grad=True)\n",
    "    model.epoch = 0\n",
    "    model.loss = 0\n",
    "    \n",
    "#     if args.cuda: model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    save_path = os.path.join(args.save_dir, 'best.pt')   \n",
    "    if os.path.exists(save_path):\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])        \n",
    "        model.loss = checkpoint['loss']\n",
    "        model.w1 = checkpoint['w1']\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.best_acc = checkpoint['best_acc']\n",
    "        model.eval()\n",
    "        print('load exist model',save_path,'best acc:',model.best_acc)  \n",
    "  \n",
    "    return model,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    for k in range(1,40):\n",
    "        args.train_file = 'train{}.csv'.format(k)\n",
    "        train_iter, dev_iter = load_data(args) \n",
    "        print('加载数据完成:{}'.format(args.train_file))\n",
    "        train(args,train_iter,dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "预测\n",
    "'''\n",
    "def predict_model(model,query,replys):\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(len(replys)): \n",
    "        f1 = query\n",
    "        f2 = replys[i].unsqueeze(dim=0)\n",
    "        logits = model(f1,f2)\n",
    "        pre = logits[0][1]\n",
    "        predict.append(pre)\n",
    "\n",
    "    m = torch.max(torch.tensor(predict))\n",
    "    for k in range(len(predict)):\n",
    "        if predict[k]==m:\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(ones,fixlen):\n",
    "    batch = []\n",
    "    for one in ones:\n",
    "        t = []\n",
    "        words = one.split()\n",
    "        for w in words:\n",
    "            if w in args.stoi.keys():\n",
    "                t.append(args.stoi[w])\n",
    "            else:\n",
    "                t.append(args.stoi[0])\n",
    "        batch.append(t[:fixlen])\n",
    "    \n",
    "    for one in batch:\n",
    "        for k in range(len(one),fixlen):\n",
    "            one.append(0)\n",
    "            \n",
    "    return torch.tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    query = '是 我们 专业 的 宿舍 啊  你 是 我们 班 的 吗 曾经 是 有种 好 伤感 的 感觉 '\n",
    "    replys = ('我 中午 哪肿 么 啦',\n",
    "                           '是 的',\n",
    "                           '啊 偶 抚摸',\n",
    "                              '谢谢 啦', \n",
    "    '棒棒 的 你',\n",
    "    '哈哈  我 一直 在 啊', \n",
    "    '我 没有 在 长宁  打钱 嘛  手机 版 的', \n",
    "    '嗯 嗯', \n",
    "    '嗯 啊', \n",
    "    '是 的 呀  这 周末 回去 ')\n",
    "    f1 = build_vocab([query],args.context_len)    \n",
    "    f2 = build_vocab(replys,args.context_len)         \n",
    "    index = predict_model(model,f1,f2)\n",
    "    print(index,replys[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对分词后的seq_context.txt，seq_replies.txt进行预测,生成结果txt\n",
    "'''\n",
    "def do_predict_work():\n",
    "    context_path = 'data/seq_context_split.txt'\n",
    "    reply_path = 'data/seq_replies_split.txt'\n",
    "    out_path = 'data/result.txt'\n",
    "\n",
    "#     load_vocab(args)\n",
    "#     model,optimizer = loadModel(args) \n",
    "\n",
    "    with open(context_path, 'r',encoding='utf-8') as file:\n",
    "        tmp = file.read()\n",
    "    context = tmp.split('\\n\\n')\n",
    "\n",
    "    with open(reply_path, 'r',encoding='utf-8') as file:\n",
    "        tmp = file.read()\n",
    "    replys = tmp.split('\\n\\n')\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for i in range(len(context)):\n",
    "        c = context[i].replace('\\n','').strip()\n",
    "        rs = replys[i].split('\\n')\n",
    "        f1 = build_vocab([c],args.context_len)    \n",
    "        f2 = build_vocab(rs,args.context_len)         \n",
    "        index = predict_model(model,f1,f2)\n",
    "        result.append(index)\n",
    "        if i%10==0:\n",
    "            sys.stdout.write('\\rpredict {}/{}\\r'.format(i,len(context)))\n",
    "#         print('Q:',i,c)\n",
    "#         print('A: ',index,rs[index],'\\n')\n",
    "        \n",
    "    with open(out_path, 'w',encoding='utf-8') as file:\n",
    "        for idx in result:\n",
    "            file.write(str(idx)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#预测测试\n",
    "# predict_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#手动修改学习速率\n",
    "# args.lr=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#手动保存模型\n",
    "# save(model,optimizer,args.save_dir, 'best', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载词汇表\n",
    "load_vocab(args)\n",
    "#加载模型，不存在则新建\n",
    "model,optimizer = loadModel(args) \n",
    "#启动训练线程\n",
    "t = threading.Thread(target=train_loop,args=[])\n",
    "t.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'pad', '一一', '一一二', '一一列举', '一一对', '一一对应', '一一记', '一一道来', '一丁']\n",
      "vocab size :  416160\n",
      "load exist model model_dir\\best.pt best acc: tensor(51.0646)\n",
      "predict 5270/10001\r"
     ]
    }
   ],
   "source": [
    "#对分词后的seq_context.txt，seq_replies.txt进行预测,生成结果txt\n",
    "do_predict_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
